{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "wgGywTOdWdAr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ftfy in /Users/home/opt/anaconda3/lib/python3.9/site-packages (6.1.1)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /Users/home/opt/anaconda3/lib/python3.9/site-packages (from ftfy) (0.2.5)\n",
      "Requirement already satisfied: sentencepiece in /Users/home/opt/anaconda3/lib/python3.9/site-packages (0.1.97)\n",
      "Requirement already satisfied: torch_optimizer in /Users/home/opt/anaconda3/lib/python3.9/site-packages (0.3.0)\n",
      "Requirement already satisfied: pytorch-ranger>=0.1.1 in /Users/home/opt/anaconda3/lib/python3.9/site-packages (from torch_optimizer) (0.1.1)\n",
      "Requirement already satisfied: torch>=1.5.0 in /Users/home/opt/anaconda3/lib/python3.9/site-packages (from torch_optimizer) (1.12.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/home/opt/anaconda3/lib/python3.9/site-packages (from torch>=1.5.0->torch_optimizer) (4.1.1)\n",
      "Requirement already satisfied: tensorflow-addons in /Users/home/opt/anaconda3/lib/python3.9/site-packages (0.18.0)\n",
      "Requirement already satisfied: packaging in /Users/home/opt/anaconda3/lib/python3.9/site-packages (from tensorflow-addons) (21.3)\n",
      "Requirement already satisfied: typeguard>=2.7 in /Users/home/opt/anaconda3/lib/python3.9/site-packages (from tensorflow-addons) (2.13.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/home/opt/anaconda3/lib/python3.9/site-packages (from packaging->tensorflow-addons) (3.0.4)\n",
      "Requirement already satisfied: torch in /Users/home/opt/anaconda3/lib/python3.9/site-packages (1.12.1)\n",
      "Requirement already satisfied: torchvision in /Users/home/opt/anaconda3/lib/python3.9/site-packages (0.13.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/home/opt/anaconda3/lib/python3.9/site-packages (from torch) (4.1.1)\n",
      "Requirement already satisfied: numpy in /Users/home/opt/anaconda3/lib/python3.9/site-packages (from torchvision) (1.21.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/home/opt/anaconda3/lib/python3.9/site-packages (from torchvision) (9.0.1)\n",
      "Requirement already satisfied: requests in /Users/home/opt/anaconda3/lib/python3.9/site-packages (from torchvision) (2.27.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/home/opt/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/home/opt/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/home/opt/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/home/opt/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (2.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install ftfy\n",
    "!pip install -q transformers\n",
    "!pip install sentencepiece\n",
    "!pip install torch_optimizer\n",
    "!pip install tensorflow-addons\n",
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -q transformers\n",
    "#pip install ftfy\n",
    "#pip install tensorflow-addons\n",
    "#pip install torch-optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "IA5ulZJgr0JO"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from torch.nn import BCEWithLogitsLoss, BCELoss\n",
    "from transformers import *\n",
    "import os, re, string\n",
    "import numpy as np\n",
    "import pickle, statistics\n",
    "from tensorflow.keras.layers import *\n",
    "from ftfy import fix_text\n",
    "from matplotlib import pyplot\n",
    "from sklearn.metrics import *\n",
    "from tqdm import tqdm, trange\n",
    "import warnings\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow_addons as tfa\n",
    "import torch_optimizer as optim\n",
    "from sklearn.metrics import *\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "jqTDegKl3aDo"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "\n",
    "path_train_en ='./Dataset/En_Train.txt'\n",
    "path_test_en ='./Dataset/En_Test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "WUBW0mN_yIQS"
   },
   "outputs": [],
   "source": [
    "class Review():\n",
    "    def __init__(self,stt,text,label):\n",
    "        self.stt = stt\n",
    "        self.labels = label\n",
    "        self.orginalText = text\n",
    "\n",
    "def read_files(filename):\n",
    "    documents = list()\n",
    "    count = 0\n",
    "    with open(filename,'r',encoding='utf8') as file:\n",
    "        text1 = file.read()\n",
    "        for item in text1.split('\\n'):\n",
    "            if item != '':\n",
    "                item = item.strip()\n",
    "                if count == 0:\n",
    "                    stt = fix_text(item)\n",
    "                    count += 1\n",
    "                elif count == 1:\n",
    "                    text = fix_text(item)\n",
    "                    count += 1\n",
    "                elif count == 2:\n",
    "                    labels = fix_text(item).replace(\"STYLE&OPTIONS\", \"STYLE_OPTIONS\")\n",
    "                    count = 0\n",
    "                    reviewtemp = Review(stt, text, labels)\n",
    "                    documents.append(reviewtemp)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "B76j0LCsd6Bz"
   },
   "outputs": [],
   "source": [
    "import string,re\n",
    "def clean_doc(text):\n",
    "  text = text.lower()\n",
    "  text = text.translate(text.maketrans('', '', string.punctuation))\n",
    "  text = re.sub(r\"[0-9]+\", \" num \", text)\n",
    "  text = re.sub('\\\\s+',' ',text)\n",
    "  text = text.strip()\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "fyIQeZi-WdA4"
   },
   "outputs": [],
   "source": [
    "# Function convert label to vector to fit model. \n",
    "# Consider each aspect as one vector. Example vector for an aspect not being mentioned is: [1,0,0,0]\n",
    "# Vector for mentioned positive aspect: [0,1,0,0]. Vector for mentioned. negative aspect : [0,0,0,1]\n",
    "def to_category_vector_aspect_polarity(label):\n",
    "    vector = np.zeros(4).astype(np.float64)\n",
    "    if label.strip() != '':\n",
    "        if 'positive' in label:\n",
    "            vector[1] = 1.0\n",
    "        elif 'neutral' in label:\n",
    "            vector[2] = 1.0\n",
    "        elif 'negative' in label:\n",
    "            vector[3] = 1.0\n",
    "        else:\n",
    "            vector[0] = 1.0\n",
    "    else:\n",
    "        vector[0] = 1.0\n",
    "    return vector\n",
    "\n",
    "import re\n",
    "def create_output_aspect_polarity(labels,categories,list_output):\n",
    "    for i,category in enumerate(categories):\n",
    "        if category in labels:\n",
    "            try:\n",
    "                output = re.findall('('+category+',\\s(positive|negative|neutral))',labels)[0][0]\n",
    "            except:\n",
    "                output = ''\n",
    "            list_output[i].append(to_category_vector_aspect_polarity(output))\n",
    "        else:\n",
    "            list_output[i].append(to_category_vector_aspect_polarity(''))\n",
    "    return list_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "dWLlW4jbDwhg"
   },
   "outputs": [],
   "source": [
    "def read_data_train(languages):\n",
    "    data  = []\n",
    "    if \"en\" in languages:\n",
    "      # English\n",
    "      for review in read_files(path_train_en):\n",
    "          data.append(review)\n",
    "\n",
    "    print(len(data))\n",
    "    return data\n",
    "\n",
    "def read_data_test(target_language):\n",
    "    data = []\n",
    "    if \"en\" in target_language:\n",
    "      for review in read_files(path_test_en):\n",
    "          data.append(review)\n",
    "\n",
    "    print(len(data))    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "BtK3Ez_sbd4s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1708\n",
      "587\n",
      "1708 587\n"
     ]
    }
   ],
   "source": [
    "listLabel = \"RESTAURANT#GENERAL,SERVICE#GENERAL,FOOD#QUALITY,FOOD#STYLE_OPTIONS,DRINKS#STYLE_OPTIONS,DRINKS#PRICES,RESTAURANT#PRICES,RESTAURANT#MISCELLANEOUS,AMBIENCE#GENERAL,FOOD#PRICES,LOCATION#GENERAL,DRINKS#QUALITY\"\n",
    "categories = listLabel.split(',')\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "output_train1 = []\n",
    "output_train2 = []\n",
    "output_train3 = []\n",
    "output_train4 = []\n",
    "output_train5 = []\n",
    "output_train6 = []\n",
    "output_train7 = []\n",
    "output_train8 = []\n",
    "output_train9 = []\n",
    "output_train10 = []\n",
    "output_train11 = []\n",
    "output_train12 = []\n",
    "\n",
    "list_output_train = list()\n",
    "list_output_train.append(output_train1)\n",
    "list_output_train.append(output_train2)\n",
    "list_output_train.append(output_train3)\n",
    "list_output_train.append(output_train4)\n",
    "list_output_train.append(output_train5)\n",
    "list_output_train.append(output_train6)\n",
    "list_output_train.append(output_train7)\n",
    "list_output_train.append(output_train8)\n",
    "list_output_train.append(output_train9)\n",
    "list_output_train.append(output_train10)\n",
    "list_output_train.append(output_train11)\n",
    "list_output_train.append(output_train12)\n",
    "\n",
    "y_train_category = []\n",
    "for review in read_data_train(\"en\"):\n",
    "    x_train.append(clean_doc(review.orginalText))\n",
    "    y_train = create_output_aspect_polarity(review.labels, categories,list_output_train)\n",
    "\n",
    "x_test = []\n",
    "y_test_category = []\n",
    "for review in read_data_test(\"en\"):\n",
    "    x_test.append(clean_doc(review.orginalText))\n",
    "\n",
    "print(len(x_train), len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "meaPLTCJvldZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load English model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "604795a35a9b42cba73361c4d1cc2ce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6b99a2509154b68879e1f3748fc8283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d041540c3e94b75a763763f2168e5cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "514f60f872c345feaf0d0dc6a9af35ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-24 16:37:11.765082: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# RobERTA MODEL\n",
    "# Reference: https://arxiv.org/abs/1907.11692\n",
    "language = \"en\"\n",
    "print(\"Load English model\")\n",
    "model_name = 'roberta-base'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name, do_lower_case=False)\n",
    "config = RobertaConfig.from_pretrained(model_name, output_hidden_states=True)\n",
    "transformer_model = TFRobertaModel.from_pretrained(model_name, config = config,from_pt=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "QlcoaUypB6-T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1708\n",
      "1708\n",
      "74\n"
     ]
    }
   ],
   "source": [
    "train_encodings = tokenizer(x_train, truncation=True, padding=True)\n",
    "print(len(train_encodings[\"input_ids\"]))\n",
    "print(len(train_encodings[\"attention_mask\"]))\n",
    "max_length = len(train_encodings[\"input_ids\"][0])\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "buRet_DdvuT4"
   },
   "outputs": [],
   "source": [
    "train_ids = np.array(train_encodings[\"input_ids\"])\n",
    "train_masks = np.array(train_encodings[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "WnmPqZvKzoxo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n"
     ]
    }
   ],
   "source": [
    "test_encodings = tokenizer(x_test, max_length=max_length, truncation=True, padding=\"max_length\")\n",
    "print(max_length)\n",
    "test_ids = np.array(test_encodings[\"input_ids\"])\n",
    "test_masks = np.array(test_encodings[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "Ywt07Q3crzFI"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "def build_model(max_length=max_length):\n",
    "      input_ids_in = tf.keras.layers.Input(shape=(max_length,), name='input_ids', dtype='int32')\n",
    "      input_masks_in = tf.keras.layers.Input(shape=(max_length,), name='input_mask', dtype='int32') \n",
    "\n",
    "      outputs = transformer_model(input_ids_in,attention_mask = input_masks_in)[0]\n",
    "\n",
    "      cls_token = outputs[:,0,:]\n",
    "\n",
    "      output1 = tf.keras.layers.Dense(4,name=\"output1\", activation='softmax')(cls_token)\n",
    "      output2 = tf.keras.layers.Dense(4,name=\"output2\", activation='softmax')(cls_token)\n",
    "      output3 = tf.keras.layers.Dense(4,name=\"output3\", activation='softmax')(cls_token)\n",
    "      output4 = tf.keras.layers.Dense(4,name=\"output4\", activation='softmax')(cls_token)\n",
    "      output5 = tf.keras.layers.Dense(4,name=\"output5\", activation='softmax')(cls_token)\n",
    "      output6 = tf.keras.layers.Dense(4,name=\"output6\", activation='softmax')(cls_token)\n",
    "      output7 = tf.keras.layers.Dense(4,name=\"output7\", activation='softmax')(cls_token)\n",
    "      output8 = tf.keras.layers.Dense(4,name=\"output8\", activation='softmax')(cls_token)\n",
    "      output9 = tf.keras.layers.Dense(4,name=\"output9\", activation='softmax')(cls_token)\n",
    "      output10 = tf.keras.layers.Dense(4,name=\"output10\", activation='softmax')(cls_token)\n",
    "      output11 = tf.keras.layers.Dense(4,name=\"output11\", activation='softmax')(cls_token)\n",
    "      output12 = tf.keras.layers.Dense(4,name=\"output12\", activation='softmax')(cls_token)\n",
    "\n",
    "      model = tf.keras.Model(inputs=[input_ids_in,input_masks_in], outputs=[output1,output2,output3,output4,output5,output6,output7,output8,output9,output10,output11,output12])\n",
    "\n",
    "      opt = tf.keras.optimizers.Adam(learning_rate=5e-5,amsgrad=True)\n",
    "      checkpoint_filepath = './checkpoint'\n",
    "      model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "          filepath=checkpoint_filepath,\n",
    "          save_weights_only=True,\n",
    "          monitor='loss',\n",
    "          mode='min',\n",
    "          save_best_only=True)\n",
    "      #opt = tfa.optimizers.AdamW(weight_decay = 0.01, learning_rate = 5e-5)\n",
    "      loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "      metric = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "      model.compile(loss={'output1':loss,'output2':loss,'output3':loss,'output4':loss,'output5':loss,'output6':loss,'output7':loss,'output8':loss,'output9':loss,'output10':loss,'output11':loss,'output12':loss},optimizer=opt, metrics = [metric])\n",
    "      print(model.summary())\n",
    "        \n",
    "      list_total_Y_train  = [np.array(list_output_train[0]),np.array(list_output_train[1]),np.array(list_output_train[2]),np.array(list_output_train[3]),np.array(list_output_train[4]),np.array(list_output_train[5]),np.array(list_output_train[6]),np.array(list_output_train[7]),np.array(list_output_train[8]),np.array(list_output_train[9]),np.array(list_output_train[10]),np.array(list_output_train[11])]\n",
    "              \n",
    "      history = model.fit([train_ids,np.array(train_masks)], list_total_Y_train,batch_size=32, epochs=50,callbacks=[model_checkpoint_callback])\n",
    "      model.load_weights(checkpoint_filepath)\n",
    "      return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyxQexXBZdFp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 74)]         0           []                               \n",
      "                                                                                                  \n",
      " input_mask (InputLayer)        [(None, 74)]         0           []                               \n",
      "                                                                                                  \n",
      " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n",
      " el)                            thPoolingAndCrossAt               'input_mask[0][0]']             \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 74,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=(                                               \n",
      "                                (None, 74, 768),                                                  \n",
      "                                 (None, 74, 768),                                                 \n",
      "                                 (None, 74, 768),                                                 \n",
      "                                 (None, 74, 768),                                                 \n",
      "                                 (None, 74, 768),                                                 \n",
      "                                 (None, 74, 768),                                                 \n",
      "                                 (None, 74, 768),                                                 \n",
      "                                 (None, 74, 768),                                                 \n",
      "                                 (None, 74, 768),                                                 \n",
      "                                 (None, 74, 768),                                                 \n",
      "                                 (None, 74, 768),                                                 \n",
      "                                 (None, 74, 768),                                                 \n",
      "                                 (None, 74, 768)),                                                \n",
      "                                 attentions=None, c                                               \n",
      "                                ross_attentions=Non                                               \n",
      "                                e)                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_roberta_model[0][13]']      \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " output1 (Dense)                (None, 4)            3076        ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " output2 (Dense)                (None, 4)            3076        ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " output3 (Dense)                (None, 4)            3076        ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " output4 (Dense)                (None, 4)            3076        ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " output5 (Dense)                (None, 4)            3076        ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " output6 (Dense)                (None, 4)            3076        ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " output7 (Dense)                (None, 4)            3076        ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " output8 (Dense)                (None, 4)            3076        ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " output9 (Dense)                (None, 4)            3076        ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " output10 (Dense)               (None, 4)            3076        ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " output11 (Dense)               (None, 4)            3076        ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " output12 (Dense)               (None, 4)            3076        ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 124,682,544\n",
      "Trainable params: 124,682,544\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "54/54 [==============================] - 1054s 19s/step - loss: 4.9488 - output1_loss: 0.7120 - output2_loss: 0.7620 - output3_loss: 0.8463 - output4_loss: 0.3792 - output5_loss: 0.1870 - output6_loss: 0.1675 - output7_loss: 0.3321 - output8_loss: 0.3352 - output9_loss: 0.5264 - output10_loss: 0.2958 - output11_loss: 0.1813 - output12_loss: 0.2241 - output1_accuracy: 0.7213 - output2_accuracy: 0.7330 - output3_accuracy: 0.6651 - output4_accuracy: 0.8923 - output5_accuracy: 0.9397 - output6_accuracy: 0.9397 - output7_accuracy: 0.9005 - output8_accuracy: 0.9415 - output9_accuracy: 0.8331 - output10_accuracy: 0.9356 - output11_accuracy: 0.9444 - output12_accuracy: 0.9239\n",
      "Epoch 2/50\n",
      "54/54 [==============================] - 1101s 20s/step - loss: 2.6999 - output1_loss: 0.4035 - output2_loss: 0.3383 - output3_loss: 0.5167 - output4_loss: 0.2674 - output5_loss: 0.0678 - output6_loss: 0.0610 - output7_loss: 0.1741 - output8_loss: 0.2319 - output9_loss: 0.2648 - output10_loss: 0.1846 - output11_loss: 0.0851 - output12_loss: 0.1047 - output1_accuracy: 0.8583 - output2_accuracy: 0.8905 - output3_accuracy: 0.8267 - output4_accuracy: 0.9239 - output5_accuracy: 0.9836 - output6_accuracy: 0.9889 - output7_accuracy: 0.9567 - output8_accuracy: 0.9456 - output9_accuracy: 0.9139 - output10_accuracy: 0.9514 - output11_accuracy: 0.9836 - output12_accuracy: 0.9731\n",
      "Epoch 3/50\n",
      "40/54 [=====================>........] - ETA: 5:37 - loss: 1.8859 - output1_loss: 0.2771 - output2_loss: 0.1893 - output3_loss: 0.3563 - output4_loss: 0.1969 - output5_loss: 0.0496 - output6_loss: 0.0536 - output7_loss: 0.1337 - output8_loss: 0.1736 - output9_loss: 0.1930 - output10_loss: 0.1306 - output11_loss: 0.0614 - output12_loss: 0.0708 - output1_accuracy: 0.9141 - output2_accuracy: 0.9406 - output3_accuracy: 0.8680 - output4_accuracy: 0.9328 - output5_accuracy: 0.9844 - output6_accuracy: 0.9844 - output7_accuracy: 0.9641 - output8_accuracy: 0.9523 - output9_accuracy: 0.9414 - output10_accuracy: 0.9602 - output11_accuracy: 0.9852 - output12_accuracy: 0.9742"
     ]
    }
   ],
   "source": [
    "# Save result in folder Result\n",
    "import operator, random\n",
    "path_save = './Result/'\n",
    "\n",
    "seed = 42\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "os.environ['PYTHONHASHSEED']=str(seed)\n",
    "tf.random.set_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "model = build_model()\n",
    "count = 0\n",
    "textPrint = ''\n",
    "textREsult = []\n",
    "for index,item in enumerate(test_ids):\n",
    "    predicted = model.predict([np.expand_dims(item, axis=0),np.expand_dims(np.array(test_masks[index]), axis=0)])\n",
    "    s = ''\n",
    "    for i, predict in enumerate(predicted):\n",
    "        index2, value = max(enumerate(predict[0]), key=operator.itemgetter(1))\n",
    "        if index2 == 1:\n",
    "            s+= '{' + str(categories[i]) + ', positive}, '\n",
    "        elif index2 == 2:\n",
    "            s+= '{' + str(categories[i]) + ', neutral}, '\n",
    "        elif index2 == 3:\n",
    "            s+= '{' + str(categories[i]) + ', negative}, '\n",
    "    if s.strip() == \"\":\n",
    "      s = \"{RESTAURANT#GENERAL, neutral}, \"\n",
    "    textPrint += '#' + str(count +1)+'\\n'\n",
    "    textPrint += x_test[index] + '\\n'\n",
    "    textPrint += s[:len(s)-2] +'\\n\\n'\n",
    "    count +=1\n",
    "with open(path_save + 'MonoRoBERTa.txt','w',encoding = 'utf8') as file:\n",
    "    file.write(textPrint)\n",
    "with open('MonoRoBERTa.txt','w',encoding = 'utf8') as file:\n",
    "    file.write(textPrint)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OR-p1oRerero"
   },
   "outputs": [],
   "source": [
    "# Evaluation model\n",
    "!cp \"EvaluationSystemByFile.py\" \"EvaluationSystemByFile.py\"\n",
    "!cp 'Dataset/En_Test.txt' \"En_Test.txt\"\n",
    "\n",
    "print(\"Result of model on Micro F1-score\")\n",
    "!python EvaluationSystemByFile.py \"En_Test.txt\" \"MonoRoBERTa.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wmo9amP9qh_p"
   },
   "outputs": [],
   "source": [
    "#Save model to path\n",
    "path_save_model = './Model/'\n",
    "\n",
    "model.save(path_save_model + \"RoBertTa_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CIdOjC_ou4FR"
   },
   "outputs": [],
   "source": [
    "# Reference\n",
    "# 1. https://aclanthology.org/D18-1139.pdf\n",
    "# 2. https://www.researchgate.net/publication/335872705_Joint_Learning_for_Aspect_Category_Detection_and_Sentiment_Analysis_in_Chinese_Reviews\n",
    "# 3. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
